# Prometheus-Eval Environment Configuration
# Copy this file to .env and populate with your actual values

# =============================================================================
# LLM API KEYS
# =============================================================================

# OpenAI API Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_ORG_ID=your_openai_org_id_here  # Optional

# Anthropic API Configuration
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# HuggingFace API Configuration (for model downloads and inference)
HUGGINGFACE_API_TOKEN=your_huggingface_token_here  # Optional, for gated models

# =============================================================================
# INFERENCE ENGINE SETTINGS
# =============================================================================

# Default model configurations
DEFAULT_OPENAI_MODEL=gpt-5-nano
DEFAULT_ANTHROPIC_MODEL=claude-3-sonnet-20240229
DEFAULT_TEMPERATURE=0.7
DEFAULT_MAX_TOKENS=2048

# Rate limiting (requests per minute)
OPENAI_RPM_LIMIT=60
ANTHROPIC_RPM_LIMIT=50

# Timeout settings (seconds)
LLM_REQUEST_TIMEOUT=30
LLM_RETRY_ATTEMPTS=3

# =============================================================================
# EMBEDDING MODEL SETTINGS
# =============================================================================

# For BERTScore and semantic metrics
EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2
EMBEDDING_DEVICE=cpu  # Options: cpu, cuda, mps
EMBEDDING_BATCH_SIZE=32

# =============================================================================
# CODE EXECUTION SANDBOX
# =============================================================================

# Docker configuration for Pass@k evaluation
DOCKER_TIMEOUT=10  # seconds per code execution
DOCKER_MEMORY_LIMIT=512m
DOCKER_CPU_QUOTA=50000  # 0.5 CPU core

# =============================================================================
# DATA STORAGE
# =============================================================================

# Experiment data storage path
EXPERIMENT_DATA_DIR=./data/experiments
BENCHMARK_DATA_DIR=./data/benchmarks

# Results format
RESULTS_FORMAT=json  # Options: json, csv, parquet

# =============================================================================
# LOGGING
# =============================================================================

# Logging level
LOG_LEVEL=INFO  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_FILE=./logs/prometheus_eval.log

# =============================================================================
# VARIATOR (PROMPT GENERATOR) SETTINGS
# =============================================================================

# Paraphrasing LLM (for prompt variations)
PARAPHRASE_MODEL=gpt-4o-mini
PARAPHRASE_TEMPERATURE=0.9

# Emotional prompting intensity (1-10)
EMOTIONAL_INTENSITY_DEFAULT=5

# =============================================================================
# METRIC COMPUTATION SETTINGS
# =============================================================================

# BLEU settings
BLEU_MAX_NGRAM=4
BLEU_SMOOTHING=true

# BERTScore settings
BERTSCORE_LANG=en
BERTSCORE_RESCALE_WITH_BASELINE=true

# Semantic Stability settings
SEMANTIC_STABILITY_RUNS=5  # Number of inference runs for variance calculation

# Pass@k settings
PASS_AT_K_VALUES=1,5,10  # Comma-separated k values

# G-Eval settings (Phase 2)
# GEVAL_JUDGE_MODEL=gpt-4
# GEVAL_COT_STEPS=3

# =============================================================================
# VISUALIZATION SETTINGS
# =============================================================================

# Dashboard port (Phase 3)
DASHBOARD_PORT=3000
DASHBOARD_HOST=localhost

# Chart export settings
CHART_EXPORT_DPI=300
CHART_EXPORT_FORMAT=png  # Options: png, svg, pdf

# =============================================================================
# DEVELOPMENT SETTINGS
# =============================================================================

# Enable debug mode
DEBUG=false

# Mock LLM responses for testing (uses fixtures instead of real API calls)
USE_MOCK_LLM=false

# Cache LLM responses
ENABLE_CACHE=true
CACHE_DIR=./.cache

# =============================================================================
# SECURITY
# =============================================================================

# Disable code execution sandbox (NOT RECOMMENDED for production)
DISABLE_SANDBOX=false

# Maximum prompt length (characters)
MAX_PROMPT_LENGTH=10000
